{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will be exploring the car dataset and analyzing their fuel efficiency. <br >\n",
    "Specifically, we will do some exploratory analysis with visualizations, then we will build a model for Simple Linear Regression, a model for Polynomial Regression, and one model for Logistic Regression. <br >\n",
    "**The given dataset is already modified and cleaned**, but you can find [the original information here.](https://archive.ics.uci.edu/ml/datasets/auto+mpg).\n",
    "\n",
    "## Dataset Attribute Information\n",
    "\n",
    "1. **mpg**: Miles per gallon. This is one primary measurement for car fuel efficiency.\n",
    "2. **displacement** : The cylinder volumes in cubic inches.\n",
    "3. **horsepower** : Engine power.\n",
    "4. **weight** : In pounds.\n",
    "5. **acceleration** : The elapsed time in seconds to go from 0 to 60mph.\n",
    "6. **origin** : Region of origin.\n",
    "\n",
    "### Libraries that can be used: numpy, pandas, scikit-learn, seaborn, plotly, matplotlib\n",
    "Any libraries used in the discussion materials are also allowed.\n",
    "\n",
    "#### Other Notes\n",
    " - Don't worry about not being able to achieve high accuracy, it is neither the goal nor the grading standard of **this** assignment. <br >\n",
    " - If not specified, you are not required to do hyperparameter tuning, but feel free to do so if you'd like.\n",
    " - Discussion materials should be helpful for doing the assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "## Exercise 1 - Exploratory Analysis (20 points in total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1 - Correlation Matrix (10 points)\n",
    "Generate a Pearson [correlation matrix plot](https://heartbeat.fritz.ai/seaborn-heatmaps-13-ways-to-customize-correlation-matrix-visualizations-f1c49c816f07) in the form of a heatmap. See the link to have an idea about what this visualization should look like. <br >\n",
    "After generating the plot, answer the following question: <br >\n",
    "**If we are going to predict ``mpg`` in Simple Linear Regression(i.e., $y=ax+b$), which attribute are you most UNLIKELY to pick as the independent variable? Explain why.**\n",
    "\n",
    "Requirements & notes\n",
    " - When computing correlation, make sure to drop the column ``origin`` to avoid errors.\n",
    " - The computed correlation values should be shown on the plot.\n",
    " - Use a diverging color scale with the color range being \\[-1, 1\\] and center being 0 (if applicable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import seaborn\n",
    "import numpy\n",
    "import pandas\n",
    "import matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2 - Pairplot (10 points)\n",
    "Generate a pairplot(a.k.a. scatter plot matrix) of the given dataset. <br >\n",
    "After generating the plot, answer the following question: <br >\n",
    "**If we are using ``horsepower`` to predict ``mpg``, which method could lead to the best performance? (Linear Regression, Polynomial Regression, or Logistic Regression) Explain why.**\n",
    "\n",
    "Note that there is no requirement on the diagonals. You can leave empty or use other representations based on your preference. However, having ``origin``-based grouped data distributions on the diagonals effectively helps you answer some questions in the later exercises.   \n",
    "\n",
    "Requirements\n",
    " - The points should be colored based on the column ``origin``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Linear and Polynomial Regression (30 points in total)\n",
    "\n",
    "### Exercise 2.1 - Splitting Dataset (5 points)\n",
    "Split the data into training and testing set with the ratio of 80:20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2 - Simple Linear Regression (10 points)\n",
    "Using one of the other attributes(excluding ``origin``) by your choice, please build a simple linear regression model that predicts ``mpg``. <br >\n",
    "\n",
    "Requirements\n",
    " - Report the testing MSE error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3 - Polynomial Regression (15 points)\n",
    "Build polynomial regression models that predict ``mpg`` with the same choice in 2.2. <br >\n",
    "Specifically, from degree=2 to degree=4, build one respectively. <br >\n",
    "Then, based on the reported errors from only these three degrees, **do you think there is a sign of overfitting? Provide your reasoning.**\n",
    "\n",
    "\n",
    "Requirements\n",
    " - Report the training MSE error for each of the three degrees.\n",
    " - Report the testing MSE error for each of the three degrees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - Overfitting and Underfitting (25 points in total)\n",
    "The fitting dataset contains the actual train and test data spread for a model along with three rotations of the same. The dataset is provided in the Canvas file.\n",
    "\n",
    "### Exercise 3.1 - sse and variance\n",
    "Calculate the sse and variance for the three predictions based on the actual data.<br >\n",
    "Show the calculation for the above metrics.<br >\n",
    "Highlight the values you get for all three predictions and the actual data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 3.2 - Justification\n",
    "Based on the values calculated above classify the predictions into three categories base prediction, overfitting prediction, underfitting prediction. Also provide appropriate justifications for the classifications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - Outliers (25 points in total)\n",
    "Now we are going to perform outlier detection using the diabetes dataset. \n",
    "The dataset is provided in the Canvas file.\n",
    "\n",
    "### Exercise 4.1 - box plot\n",
    "Extract the 'BloodPressure' attribute from the diabetes dataset.<br >\n",
    "Create a box plot with the 'BloodPressure' attribute.<br >\n",
    "Highlight the outliers in the box plot with special colors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2 - anomaly detection\n",
    "Extract features 'BMI' and 'Insulin' from the diabetes dataset.<br >\n",
    "Implement anomaly detection using the One-Class SVM algorithm.<br >\n",
    "Plot a scatter plot similar to Lecture 2 Slide 11, annotating the outlier data points."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
