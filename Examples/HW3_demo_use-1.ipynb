{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "In this assignment, we will start with hyper-parameter tuning carried on from last homework and then building a Naïve Bayes classifier and a SVM model for the productivity satisfaction of [the given dataset](https://archive.ics.uci.edu/ml/datasets/Productivity+Prediction+of+Garment+Employees), the productivity of garment employees.\n",
    "\n",
    "## For Question 1:\n",
    "\n",
    "### About the Data Set\n",
    "Seven different types of dry beans were used in a study in Selcuk University, Turkey, taking into account the features such as form, shape, type, and structure by the market situation. A computer vision system was developed to distinguish seven different registered varieties of dry beans with similar features in order to obtain uniform seed classification. For the classification model, images of 13611 grains of 7 different registered dry beans were taken with a high-resolution camera. Bean images obtained by computer vision system were subjected to segmentation and feature extraction stages, and a total of 16 features - 12 dimensions and 4 shape forms - were obtained from the grains.\n",
    "\n",
    "Number of Instances (records in the data set): __13611__\n",
    "\n",
    "Number of Attributes (fields within each record, including the class): __17__\n",
    "\n",
    "### Data Set Attribute Information:\n",
    "\n",
    "1. __Area (A)__ : The area of a bean zone and the number of pixels within its boundaries.\n",
    "2. __Perimeter (P)__ : Bean circumference is defined as the length of its border.\n",
    "3. __Major axis length (L)__ : The distance between the ends of the longest line that can be drawn from a bean.\n",
    "4. __Minor axis length (l)__ : The longest line that can be drawn from the bean while standing perpendicular to the main axis.\n",
    "5. __Aspect ratio (K)__ : Defines the relationship between L and l.\n",
    "6. __Eccentricity (Ec)__ : Eccentricity of the ellipse having the same moments as the region.\n",
    "7. __Convex area (C)__ : Number of pixels in the smallest convex polygon that can contain the area of a bean seed.\n",
    "8. __Equivalent diameter (Ed)__ : The diameter of a circle having the same area as a bean seed area.\n",
    "9. __Extent (Ex)__ : The ratio of the pixels in the bounding box to the bean area.\n",
    "10. __Solidity (S)__ : Also known as convexity. The ratio of the pixels in the convex shell to those found in beans.\n",
    "11. __Roundness (R)__ : Calculated with the following formula: (4piA)/(P^2)\n",
    "12. __Compactness (CO)__ : Measures the roundness of an object: Ed/L\n",
    "13. __ShapeFactor1 (SF1)__\n",
    "14. __ShapeFactor2 (SF2)__\n",
    "15. __ShapeFactor3 (SF3)__\n",
    "16. __ShapeFactor4 (SF4)__\n",
    "\n",
    "17. __Classes : *Seker, Barbunya, Bombay, Cali, Dermosan, Horoz, Sira*__\n",
    "\n",
    "## For Questions 2-4:\n",
    "### Background \n",
    "The Garment Industry is one of the key examples of the industrial globalization of this modern era. It is a highly labour-intensive industry with lots of manual processes. Satisfying the huge global demand for garment products is mostly dependent on the production and delivery performance of the employees in the garment manufacturing companies. So, it is highly desirable among the decision makers in the garments industry to track, analyse and predict the productivity performance of the working teams in their factories. \n",
    "\n",
    "### Dataset Attribute Information\n",
    "\n",
    "1. **date**: Date in MM-DD-YYYY\n",
    "2. **day**: Day of the Week\n",
    "3. **quarter** : A portion of the month. A month was divided into four quarters\n",
    "4. **department** : Associated department with the instance\n",
    "5. **team_no** : Associated team number with the instance\n",
    "6. **no_of_workers** : Number of workers in each team\n",
    "7. **no_of_style_change** : Number of changes in the style of a particular product\n",
    "8. **targeted_productivity** : Targeted productivity set by the Authority for each team for each day.\n",
    "9. **smv** : Standard Minute Value, it is the allocated time for a task\n",
    "10. **wip** : Work in progress. Includes the number of unfinished items for products\n",
    "11. **over_time** : Represents the amount of overtime by each team in minutes\n",
    "12. **incentive** : Represents the amount of financial incentive (in BDT) that enables or motivates a particular course of action.\n",
    "13. **idle_time** : The amount of time when the production was interrupted due to several reasons\n",
    "14. **idle_men** : The number of workers who were idle due to production interruption\n",
    "15. **actual_productivity** : The actual % of productivity that was delivered by the workers. It ranges from 0-1.\n",
    "\n",
    "#### Libraries that can be used: numpy, scipy, pandas, scikit-learn, cvxpy, imbalanced-learn\n",
    "Any libraries used in the discussion materials are also allowed.\n",
    "\n",
    "#### Other Notes\n",
    "\n",
    " - Don't worry about not being able to achieve high accuracy, it is neither the goal nor the grading standard of this assignment. <br >\n",
    " - If not specified, you are not required to do hyperparameter tuning, but feel free to do so if you'd like.\n",
    "\n",
    "#### Trouble Shooting\n",
    "In case you have trouble installing and using imbalanced-learn(imblearn) <br >\n",
    "Run the below code cell, then go to the selection bar at top: Kernel > Restart. <br >\n",
    "Then try `import imblearn` to see if things work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "## Exercise 1 - Hyperparameter Tuning (20 points)\n",
    "\n",
    "Use either grid search or random search methodology to find the optimal number of nodes required in each hidden layer, as well as the optimal learning rate and the number of epochs, such that the accuracy of the model is maximum for the given data set.\n",
    "\n",
    "__Requirements :__\n",
    "- The set of optimal hyperparameters\n",
    "- The maximum accuracy achieved using this set of optimal hyperparameters\n",
    "\n",
    "__Note :__ Hyperparameter tuning takes a lot of time to execute. Make sure that you choose the appropriate number of each hyperparameter (preferably 3 of each), and that you allocate enough time to execute your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=MLPClassifier(activation=&#x27;logistic&#x27;, batch_size=100,\n",
       "                                     hidden_layer_sizes=(12, 3),\n",
       "                                     learning_rate_init=0.3, max_iter=500,\n",
       "                                     random_state=42, solver=&#x27;sgd&#x27;),\n",
       "             param_grid={&#x27;hidden_layer_sizes&#x27;: [(5, 7), (7, 13), (13, 10)],\n",
       "                         &#x27;learning_rate_init&#x27;: array([0.15, 0.3 ]),\n",
       "                         &#x27;max_iter&#x27;: [500, 800, 1000]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=MLPClassifier(activation=&#x27;logistic&#x27;, batch_size=100,\n",
       "                                     hidden_layer_sizes=(12, 3),\n",
       "                                     learning_rate_init=0.3, max_iter=500,\n",
       "                                     random_state=42, solver=&#x27;sgd&#x27;),\n",
       "             param_grid={&#x27;hidden_layer_sizes&#x27;: [(5, 7), (7, 13), (13, 10)],\n",
       "                         &#x27;learning_rate_init&#x27;: array([0.15, 0.3 ]),\n",
       "                         &#x27;max_iter&#x27;: [500, 800, 1000]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, batch_size=100, hidden_layer_sizes=(12, 3),\n",
       "              learning_rate_init=0.3, max_iter=500, random_state=42,\n",
       "              solver=&#x27;sgd&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, batch_size=100, hidden_layer_sizes=(12, 3),\n",
       "              learning_rate_init=0.3, max_iter=500, random_state=42,\n",
       "              solver=&#x27;sgd&#x27;)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=MLPClassifier(activation='logistic', batch_size=100,\n",
       "                                     hidden_layer_sizes=(12, 3),\n",
       "                                     learning_rate_init=0.3, max_iter=500,\n",
       "                                     random_state=42, solver='sgd'),\n",
       "             param_grid={'hidden_layer_sizes': [(5, 7), (7, 13), (13, 10)],\n",
       "                         'learning_rate_init': array([0.15, 0.3 ]),\n",
       "                         'max_iter': [500, 800, 1000]})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "dataset = pd.read_csv(\"wine.csv\")\n",
    "\n",
    "X = dataset.drop('Wine', axis = 1)\n",
    "y = dataset['Wine']\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_rescaled = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(data = X_rescaled, columns = X.columns)\n",
    "\n",
    "set_of_classes = y.value_counts().index.tolist()\n",
    "set_of_classes= pd.DataFrame({'Class': set_of_classes})\n",
    "y = pd.get_dummies(y)\n",
    "\n",
    "max_iterations = [500,800,1000]\n",
    "hidden_layer_siz = [(5, 7), (7, 13), (13, 10)]\n",
    "learning_rates = 0.15 * np.arange(1, 3)\n",
    "\n",
    "param_grid = dict(learning_rate_init = learning_rates, hidden_layer_sizes = hidden_layer_siz, max_iter = max_iterations)\n",
    "# set model\n",
    "mlp = MLPClassifier(solver = 'sgd', random_state = 42, activation = 'logistic', learning_rate_init = 0.3, batch_size = 100, hidden_layer_sizes = (12, 3), max_iter = 500)\n",
    "\n",
    "# For Grid Search\n",
    "grid = GridSearchCV(estimator = mlp, param_grid = param_grid)\n",
    "\n",
    "# For Random Search\n",
    "# grid = RandomizedSearchCV(estimator = mlp, param_distributions = param_grid, n_iter = 10)\n",
    "\n",
    "grid.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Hyper-parameters :  {'hidden_layer_sizes': (5, 7), 'learning_rate_init': 0.15, 'max_iter': 500}\n",
      "Optimal Accuracy :  0.9552380952380952\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal Hyper-parameters : \", grid.best_params_)\n",
    "print(\"Optimal Accuracy : \", grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - General Data Preprocessing (10 points)\n",
    "\n",
    "Our dataset needs cleaning before building any models. Some of the cleaning tasks are common in general, but depends on what kind of models we are building, sometimes we have to do additional processing. These additional tasks will be mentioned in each of the remaining two exercises later.\n",
    "\n",
    "Note that **we will be using this processed data from exercise 1 in each of the remaining two exercises**.\n",
    "\n",
    "For convenience, here are the attributes that we would treat them as **categorical attributes**: `day`, `quarter`, `department`, and `team`. \n",
    "\n",
    " - Drop the column `date`.\n",
    " - For each of the categorical attributes, **print out** all the unique elements.\n",
    " - For each of the categorical attributes, remap the duplicated items, if you find there are typos or spaces among the duplicated items.\n",
    "     - For example, \"a\" and \"a \" should be the same, so we need to update \"a \" to be \"a\".\n",
    "     - Another example, \"apple\" and \"appel\" should be the same, so you should update \"appel\" to be \"apple\".\n",
    "     \n",
    "\n",
    " - Create another column named `satisfied` that records the productivity performance. The behavior defined as follows. **This is the dependent variable we'd like to classify in this assignment.**\n",
    "     - Return True or 1 if `actual_productivity` is equal to or greater than `targeted_productivity`. Otherwise, return False or 0, which means the team fails to meet the expected performance.\n",
    " - Drop the columns `actual_productivity` and `targeted_productivity`.\n",
    "\n",
    "\n",
    " - Find and **print out** which columns/attributes that have empty vaules, e.g., NA, NaN, null, None.\n",
    " - Fill the empty values with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from IPython.display import display # Just for solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Male' 'Female']\n",
      "['typical angina' 'asymptomatic' 'non-anginal' 'atypical angina']\n",
      "[ True False]\n",
      "['lv hypertrophy' 'normal' 'st-t abnormality']\n",
      "[False  True]\n",
      "['downsloping' 'flat' 'upsloping']\n",
      "['fixed defect' 'normal' 'reversable defect']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('heart_disease_uci.csv')\n",
    "df = df.drop(columns=['id', 'dataset'])\n",
    "\n",
    "cats = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal']\n",
    "for cat in cats:\n",
    "    print(df[cat].unique()) \n",
    "# You should notice that we have duplicated \"finishing\" and \"finishing \"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normal              150\n",
       "lv hypertrophy      148\n",
       "st-t abnormality      4\n",
       "Name: restecg, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# typo \n",
    "df = df.replace('st-t abnormality ', 'stt abnormality') \n",
    "df['restecg'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['satisfied'] = (df['num'] >= 1)\n",
    "df = df.drop(columns=['num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ca'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns[df.isna().any()])\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(302, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalch</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>satisfied</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>Male</td>\n",
       "      <td>typical angina</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>True</td>\n",
       "      <td>lv hypertrophy</td>\n",
       "      <td>150</td>\n",
       "      <td>False</td>\n",
       "      <td>2.3</td>\n",
       "      <td>downsloping</td>\n",
       "      <td>0.0</td>\n",
       "      <td>fixed defect</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>Male</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>False</td>\n",
       "      <td>lv hypertrophy</td>\n",
       "      <td>108</td>\n",
       "      <td>True</td>\n",
       "      <td>1.5</td>\n",
       "      <td>flat</td>\n",
       "      <td>3.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>Male</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>120</td>\n",
       "      <td>229</td>\n",
       "      <td>False</td>\n",
       "      <td>lv hypertrophy</td>\n",
       "      <td>129</td>\n",
       "      <td>True</td>\n",
       "      <td>2.6</td>\n",
       "      <td>flat</td>\n",
       "      <td>2.0</td>\n",
       "      <td>reversable defect</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>Male</td>\n",
       "      <td>non-anginal</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>False</td>\n",
       "      <td>normal</td>\n",
       "      <td>187</td>\n",
       "      <td>False</td>\n",
       "      <td>3.5</td>\n",
       "      <td>downsloping</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>Female</td>\n",
       "      <td>atypical angina</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>False</td>\n",
       "      <td>lv hypertrophy</td>\n",
       "      <td>172</td>\n",
       "      <td>False</td>\n",
       "      <td>1.4</td>\n",
       "      <td>upsloping</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     sex               cp  trestbps  chol    fbs         restecg  \\\n",
       "0   63    Male   typical angina       145   233   True  lv hypertrophy   \n",
       "1   67    Male     asymptomatic       160   286  False  lv hypertrophy   \n",
       "2   67    Male     asymptomatic       120   229  False  lv hypertrophy   \n",
       "3   37    Male      non-anginal       130   250  False          normal   \n",
       "4   41  Female  atypical angina       130   204  False  lv hypertrophy   \n",
       "\n",
       "   thalch  exang  oldpeak        slope   ca               thal  satisfied  \n",
       "0     150  False      2.3  downsloping  0.0       fixed defect      False  \n",
       "1     108   True      1.5         flat  3.0             normal       True  \n",
       "2     129   True      2.6         flat  2.0  reversable defect       True  \n",
       "3     187  False      3.5  downsloping  0.0             normal      False  \n",
       "4     172  False      1.4    upsloping  0.0             normal      False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Just for showing you the look of the processed data\n",
    "print(df.shape)\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - Naïve Bayes Classifier (40 points in total)\n",
    "\n",
    "### Exercise 3.1 - Additional Data Preprocessing (10 points)\n",
    "\n",
    "To build a Naïve Bayes Classifier, we need to further encode our categorical variables.\n",
    "\n",
    " - For each of the **categorical attribtues**, encode the set of categories to be **0 ~ (n_classes - 1)**.\n",
    "     - For example, \\[\"paris\", \"paris\", \"tokyo\", \"amsterdam\"\\] should be encoded as \\[1, 1, 2, 0\\].\n",
    "     - Note that the order does not really matter, i.e., \\[0, 0, 1, 2\\] also works. But you have to start with 0 in your encodings.\n",
    "     - You can find information about this encoding in the discussion materials.\n",
    "\n",
    "\n",
    " - Split the data into training and testing set with the ratio of 80:20.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to continue the task with your processed data from Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalch</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>satisfied</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  sex   cp  trestbps   chol  fbs  restecg  thalch  exang  oldpeak  \\\n",
       "0  29.0  1.0  3.0      31.0   64.0  1.0      0.0    49.0    0.0     22.0   \n",
       "1  33.0  1.0  0.0      40.0  111.0  0.0      0.0    10.0    1.0     15.0   \n",
       "2  33.0  1.0  0.0      14.0   60.0  0.0      0.0    29.0    1.0     25.0   \n",
       "3   3.0  1.0  2.0      22.0   80.0  0.0      1.0    84.0    0.0     32.0   \n",
       "4   7.0  0.0  1.0      22.0   35.0  0.0      0.0    71.0    0.0     14.0   \n",
       "5  22.0  1.0  1.0      14.0   67.0  0.0      1.0    76.0    0.0      8.0   \n",
       "6  28.0  0.0  0.0      28.0   97.0  0.0      0.0    59.0    0.0     33.0   \n",
       "7  23.0  0.0  0.0      14.0  145.0  0.0      1.0    62.0    1.0      6.0   \n",
       "8  29.0  1.0  0.0      22.0   83.0  0.0      0.0    46.0    0.0     14.0   \n",
       "9  19.0  1.0  0.0      28.0   34.0  1.0      0.0    54.0    1.0     29.0   \n",
       "\n",
       "   slope   ca  thal  satisfied  \n",
       "0    0.0  0.0   0.0        0.0  \n",
       "1    1.0  3.0   1.0        1.0  \n",
       "2    1.0  2.0   2.0        1.0  \n",
       "3    0.0  0.0   1.0        0.0  \n",
       "4    2.0  0.0   1.0        0.0  \n",
       "5    2.0  0.0   1.0        0.0  \n",
       "6    0.0  2.0   1.0        1.0  \n",
       "7    2.0  0.0   1.0        0.0  \n",
       "8    1.0  1.0   2.0        1.0  \n",
       "9    0.0  0.0   2.0        1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_nb = df.copy()\n",
    "df_nb = pd.DataFrame(preprocessing.OrdinalEncoder().fit_transform(df_nb), columns=df_nb.columns)\n",
    "# OrdinalEncoder will skip numerical values. LabelEncoder also works as they share the same functionality.\n",
    "display(df_nb.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(241, 13) (61, 13)\n"
     ]
    }
   ],
   "source": [
    "nb_train, nb_test = train_test_split(df_nb, test_size=0.2)\n",
    "X_nb_train, y_nb_train = nb_train.drop(columns=['satisfied']), nb_train['satisfied']\n",
    "X_nb_test, y_nb_test = nb_test.drop(columns=['satisfied']), nb_test['satisfied']\n",
    "print(X_nb_train.shape, X_nb_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2 - Naïve Bayes Classifier for Categorical Attributes (15 points)\n",
    "\n",
    "Use the categorical attributes **only**, please build a Categorical Naïve Bayes classifier that predicts the column `satisfied`. <br >\n",
    "Report the **testing result** using `classification_report`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to do this task with your processed data from Exercise 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_cat = CategoricalNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3 - Naïve Bayes Classifier for Numerical Attributes (15 points)\n",
    "\n",
    "Use the numerical attributes **only**, please build a Gaussian Naïve Bayes classifier that predicts the column `satisfied`. <br >\n",
    "Report the **testing result** using `classification_report`.\n",
    "\n",
    "**Remember to scale your data. The scaling method is up to you.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to do this task with your processed data from Exercise 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.80      0.78        35\n",
      "         1.0       0.71      0.65      0.68        26\n",
      "\n",
      "    accuracy                           0.74        61\n",
      "   macro avg       0.73      0.73      0.73        61\n",
      "weighted avg       0.74      0.74      0.74        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nums = [col for col in X_nb_train.columns if col not in cats]\n",
    "\n",
    "clf_num = GaussianNB()\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_nb_train[nums])\n",
    "\n",
    "Z_nb_train = scaler.transform(X_nb_train[nums])\n",
    "Z_nb_test = scaler.transform(X_nb_test[nums])\n",
    "\n",
    "clf_num.fit(Z_nb_train, np.asarray(y_nb_train))\n",
    "\n",
    "print(classification_report(y_nb_test, clf_num.predict(Z_nb_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - SVM Classifier (40 points in total)\n",
    "\n",
    "### Exercise 4.1 - Additional Data Preprocessing (10 points)\n",
    "\n",
    "To build a SVM Classifier, we need a different encoding for our categorical variables.\n",
    "\n",
    " - For each of the **categorical attribtues**, encode them with **one-hot encoding**.\n",
    "     - You can find information about this encoding in the discussion materials.\n",
    "\n",
    "\n",
    " - Split the data into training and testing set with the ratio of 80:20.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to continue the task with your processed data from Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn solution\n",
    "cats = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal']\n",
    "nums = [col for col in df.drop(columns=['satisfied']).columns if col not in cats]\n",
    "df_svm = df.copy()\n",
    "df_svm = pd.get_dummies(df_svm, columns=cats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([      'age',  'trestbps',      'chol',    'thalch',   'oldpeak',\n",
      "              'ca', 'satisfied',           0,           1,           2,\n",
      "                 3,           4,           5,           6,           7,\n",
      "                 8,           9,          10,          11,          12,\n",
      "                13,          14,          15,          16,          17,\n",
      "                18],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_svm.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_train, svm_test = train_test_split(df_svm, test_size=0.2)\n",
    "X_svm_train, y_svm_train = svm_train.drop(columns=['satisfied']), svm_train['satisfied']\n",
    "X_svm_test, y_svm_test = svm_test.drop(columns=['satisfied']), svm_test['satisfied']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2 - SVM with Different Kernels (20 points)\n",
    "\n",
    "Using all the attributes we have, please build a SVM that predicts the column `satisfied`. <br >\n",
    "Specifically, please \n",
    " - Build one SVM with **linear kernel**.\n",
    " - Build another SVM but with **rbf kernel**.\n",
    " - Report the **testing results** of **both models** using `classification report`.\n",
    "\n",
    "The kernel is the only setting requirement. <br >\n",
    "Other hyperparameter tuning is not required. But make sure they are the same in these two SVMs if you'd like to tune the model. In other words, the only difference between the two SVMs is the kernel setting.\n",
    "\n",
    "**Remember to scale your data. The scaling method is up to you.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to do this task with your processed data from Exercise 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If all the data is scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Kernel\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.79      0.79      0.79        29\n",
      "        True       0.81      0.81      0.81        32\n",
      "\n",
      "    accuracy                           0.80        61\n",
      "   macro avg       0.80      0.80      0.80        61\n",
      "weighted avg       0.80      0.80      0.80        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc_li = SVC(kernel='linear')\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_svm_train)\n",
    "\n",
    "Z_svm_train = scaler.transform(X_svm_train)\n",
    "Z_svm_test = scaler.transform(X_svm_test)\n",
    "\n",
    "svc_li.fit(Z_svm_train, np.asarray(y_svm_train))\n",
    "\n",
    "print('Linear Kernel')\n",
    "print(classification_report(y_svm_test, svc_li.predict(Z_svm_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.3 - SVM with Over-sampling (10 points)\n",
    " - For the column `satisfied` in our **training set**, please **print out** the frequency of each class. \n",
    " - Oversample the **training data**. \n",
    " - For the column `satisfied` in the oversampled data, **print out** the frequency of each class  again.\n",
    " - Re-build the 2 SVMs with the same setting you have in Exercise 3.2, but **use oversampled training data** instead.\n",
    "     - Do not forget to scale the data first. As always, the scaling method is up to you.\n",
    " - Report the testing result with `classification_report`.\n",
    "\n",
    "You can use ANY methods listed on [here](https://imbalanced-learn.org/stable/references/over_sampling.html#) such as RandomOverSampler or SMOTE. <br > \n",
    "You are definitely welcomed to build your own oversampler. <br >\n",
    "\n",
    "Note that you do not have to over-sample your testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to do this task with your processed data from Exercise 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    134\n",
      "True     107\n",
      "Name: satisfied, dtype: int64\n",
      "True     134\n",
      "False    134\n",
      "Name: satisfied, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# So far you should've noticed the very low recall score for class False/0.\n",
    "# That is because the dataset we have is imbalanced and class False is the minority class.\n",
    "# Naïve Bayes and SVM are sensitive to the dataset imbalance.\n",
    "print(y_svm_train.value_counts())\n",
    "\n",
    "ros = RandomOverSampler()\n",
    "X_os, y_os = ros.fit_resample(X_svm_train, y_svm_train)\n",
    "\n",
    "print(y_os.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example of how to write a SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SVM:\n",
    "    def __init__(self,lr=.005,iters=1000,lambda_p=0.01):\n",
    "        self.lr=lr\n",
    "        self.iters=iters\n",
    "        self.lambda_p = lambda_p\n",
    "\n",
    "        self.weight = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        n_samples,n_feat = X.shape\n",
    "        \n",
    "        self.weight = np.zeros(n_feat)\n",
    "        self.bias = 0\n",
    "        \n",
    "        y_true = np.where(y <= 0, -1, 1) # same as -1 if y <= 0 else 1\n",
    "        \n",
    "        for _ in range(self.iters):\n",
    "            for idx, sample in enumerate(X):\n",
    "                y_pred = y_true[idx]*(np.dot(sample,self.weight)-self.bias)\n",
    "\n",
    "                if y_pred>=1:\n",
    "                    self.weight -= self.lr * (2 * self.lambda_p * self.weight)\n",
    "                else:\n",
    "                    self.weight -= self.lr * (2 * self.lambda_p * self.weight - np.dot(sample, y_true[idx]))\n",
    "                    self.bias -= self.lr * y_true[idx]\n",
    "        \n",
    "    def predict(self,X):\n",
    "        # do a forward pass\n",
    "        y = np.dot(X, self.weight) - self.bias\n",
    "        return y>=0 # if > 0, belongs to class 0, else class 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.80      0.83      0.81        29\n",
      "        True       0.84      0.81      0.83        32\n",
      "\n",
      "    accuracy                           0.82        61\n",
      "   macro avg       0.82      0.82      0.82        61\n",
      "weighted avg       0.82      0.82      0.82        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm = SVM()\n",
    "svm.fit(Z_svm_train, np.asarray(y_svm_train))\n",
    "\n",
    "print(classification_report(y_svm_test, svm.predict(Z_svm_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bf422c4b145da1f752a44ecd861d56cafcd104be98772f2367cebfe11f794fbe"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
